{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kmodels as kmk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'C:\\Users\\efons\\Desktop\\kalyn_lstm\\kmk\\data\\SeparatedData_MedFilt_v2-Copy1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EPB', 'EPL', 'FPL', 'APL', 'ADD', 'FCU', 'FPB', 'OPP']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave_out_df = df[df['Subject'] == 'RGr10']\n",
    "train_df = df[df['Subject'] != 'RGr10']\n",
    "window_size = 100\n",
    "stride = window_size\n",
    "train_data = kmk.Dataset_LSTM_experimental(train_df, window_size=window_size, stride=stride)\n",
    "test_data = kmk.Dataset_LSTM_experimental(leave_out_df, window_size=window_size, stride=stride)\n",
    "train_data.xcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcols = train_data.xcols\n",
    "xcols.append('Time')\n",
    "train_data = kmk.Dataset_LSTM_experimental(train_df, window_size=window_size, stride=stride, xcols=xcols)\n",
    "test_data = kmk.Dataset_LSTM_experimental(leave_out_df, window_size=window_size, stride=stride, xcols=xcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EPB', 'EPL', 'FPL', 'APL', 'ADD', 'FCU', 'FPB', 'OPP', 'Time'],\n",
       " torch.Size([42, 100, 9]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.xcols, train_data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = kmk.random_models_LSTM(25, batch_size_range=(2, len(train_data)), n_linear_layers_range=(2, 4))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = kmk.train_LSTMs(models, train_data, train_data, n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(results[3])), results[3]['mae'])\n",
    "# put the mae on the bars\n",
    "for i, v in enumerate(results[3]['mae']):\n",
    "    plt.text(i, v, str(round(v, 2)), color='blue', fontweight='bold')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.read_pickle(r'C:\\Users\\efons\\Desktop\\kalyn_lstm\\kmk\\data\\LP_Simulations_Formatted.pkl')\n",
    "window_size = 100\n",
    "stride = window_size\n",
    "sim_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets split the dataframe into a train and test set using the Subject column. Keep subjects separate during training and testing\n",
    "#df = dd.from_pandas(df, npartitions=4)\n",
    "subject_dfs = kmk.split_df_into_subjects(sim_df)\n",
    "train_split = 0.9\n",
    "n_train_subjects = int(len(subject_dfs) * train_split)\n",
    "n_test_subjects = len(subject_dfs) - n_train_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick the subjects to use for training and testing\n",
    "train_subjects_index = np.random.choice(len(subject_dfs), n_train_subjects, replace=False)\n",
    "train_df_sim = pd.concat([subject_dfs[i] for i in train_subjects_index])\n",
    "test_df_sim = pd.concat([subject_dfs[i] for i in range(len(subject_dfs)) if i not in train_subjects_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sim = kmk.Dataset_LSTM_opensim(train_df_sim, sort_column=['Subject'], window_size=window_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sim = kmk.Dataset_LSTM_opensim(test_df_sim, sort_column=['Subject'], window_size=window_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sim.X = torch.from_numpy(train_data_sim.X).float()\n",
    "train_data_sim.Y = torch.from_numpy(train_data_sim.Y).float()\n",
    "test_data_sim.X = torch.from_numpy(test_data_sim.X).float()\n",
    "test_data_sim.Y = torch.from_numpy(test_data_sim.Y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "ax[0].plot(train_data[1][0])\n",
    "ax[0].set_xlabel('time steps')\n",
    "ax[0].set_ylabel('Muscle Activation')\n",
    "ax[1].plot(train_data[1][1])\n",
    "ax[1].set_xlabel('time steps')\n",
    "ax[1].set_ylabel('Force')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = lstm(n_linear_layers=3, n_outputs = len(train_data.Y[0].flatten()), lstm_n_outputs = 3000, layer_size=30).to('cuda')\n",
    "best_model_index = np.argmin(results[3]['mae'])\n",
    "model = results[0]\n",
    "learning_rate = results[2]['learning_rate'][best_model_index]\n",
    "batch_size = results[2]['batch_size'][best_model_index]\n",
    "batch_size = 224\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = kmk.run_Pytorch(model, train_data_sim.X.reshape(-1, 100, 8), train_data_sim.Y, n_epochs=101, batch_size=batch_size, learning_rate=learning_rate, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLLSTM(model, X, change_layers=1):\n",
    "    new = kmk.lstm(n_linear_layers=model.n_linear_layers, n_outputs=model.output.out_features, lstm_n_outputs=model.lstm_n_outputs, layer_size=model.layer_size, hidden_size=36)\n",
    "    test = new(X)\n",
    "    new.load_state_dict(model.state_dict())\n",
    "    children = [child for child in new.children()]\n",
    "    for child in children:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    total_layers = len(children)\n",
    "    for i in range(change_layers):\n",
    "        layer = children[total_layers-i-1]\n",
    "        layer_params = layer.parameters()\n",
    "        for p in layer_params:\n",
    "            p.requires_grad = True\n",
    "    return new\n",
    "\n",
    "# makes make another TLMLSTM_newlayers function\n",
    "def TLMLSTM_newlayers(model, X, change_layers=1, new_layer_size=10):\n",
    "    new = kmk.lstm(n_linear_layers=model.n_linear_layers, \n",
    "               n_outputs=model.output.out_features, \n",
    "               lstm_n_outputs=model.lstm_n_outputs, \n",
    "               layer_size=model.layer_size)\n",
    "    test = new(X)\n",
    "    new.load_state_dict(model.state_dict())\n",
    "    children = [child for child in new.children()]\n",
    "    for child in children:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    total_layers = len(children)\n",
    "    # add the new layers\n",
    "    new.new_layer_size = new_layer_size\n",
    "    new.layers = len(new.fcs) + change_layers\n",
    "    for i in range(change_layers):\n",
    "        if i == 0:\n",
    "            new.fcs.append(nn.Linear(new.layer_size, new.new_layer_size))\n",
    "        else:\n",
    "            new.fcs.append(nn.Linear(new.new_layer_size, new.new_layer_size))\n",
    "            # make sure the new layers are trainable\n",
    "        new.fcs[-1].requires_grad = True\n",
    "    # make the new output layer \n",
    "    new.output = nn.Linear(new.new_layer_size, new.output.out_features)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLLSTM(model, train_data_sim.X[0].reshape(1, 100, 8), change_layers=3)\n",
    "tlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tlm = kmk.run_Pytorch(tlm, train_data.X, train_data.Y, n_epochs=2001, batch_size=results[2].iloc[best_model_index]['batch_size'], learning_rate=learning_rate, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to plot a paper worthy learning curve of the model. Make the line stand out in a clean cartoonish way. Make the y axis\n",
    "# log scale. \n",
    "def plot_learning_curve(losses, title='Learning Curve', xlabel='Epoch', ylabel='Loss', figsize=(10,10)):\n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yscale('log')\n",
    "    # add a grid\n",
    "    ax.grid()\n",
    "    return fig, ax\n",
    "plot_learning_curve(loss_tlm, title='Learning Curve', xlabel='Epoch', ylabel='Loss', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figname = 'FFNN_FineTuned_ExpTest.png'\n",
    "import math\n",
    "# Functions\n",
    "def roundup(x, nearest = 5):\n",
    "    return int(math.ceil(x / nearest)) * nearest\n",
    "\n",
    "def rounddown(x, nearest = 5):\n",
    "    return int(math.floor(x / nearest)) * nearest\n",
    "\n",
    "# Parity subplots\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "Y = test_data.Y.reshape(-1,3).detach().cpu().numpy()\n",
    "pred = model(test_data.X.reshape(-1, 100, 8)).detach().cpu().numpy().reshape(-1,3)\n",
    "X_true = Y[:,0]\n",
    "X_pred = pred[:,0]\n",
    "Y_true = Y[:,1]\n",
    "Y_pred = pred[:,1]\n",
    "Z_true = Y[:,2]\n",
    "Z_pred = pred[:,2]\n",
    "\n",
    "# Plot predictions of X \n",
    "ax1.scatter(X_true, X_pred, s=50, alpha=0.5, edgecolors='k', color='red')\n",
    "ax1.set_title('Distal (+)', fontsize = 20)\n",
    "ax1.set_xlabel('True Force (N)', fontsize = 20)\n",
    "ax1.set_ylabel('Predicted Force (N)', fontsize = 20)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# Set min and max range for axes\n",
    "alldata = np.concatenate((np.array(X_true), X_pred), axis=0)\n",
    "max_range = roundup(alldata.max())\n",
    "min_range = rounddown(alldata.min())\n",
    "ax1.set_xlim(min_range, max_range)\n",
    "ax1.set_ylim(min_range, max_range)\n",
    "ax1.set_xticks([min_range, max_range])\n",
    "ax1.set_yticks([min_range, max_range])\n",
    "ax1.tick_params(axis='both', which = 'major', labelsize = 16)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], '--k', linewidth=5, transform=ax1.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "# Plot predictions of Y\n",
    "ax2.scatter(Y_true, Y_pred, s=50, alpha=0.5, edgecolors='k', color = 'green')\n",
    "ax2.set_title('Dorsal (+)', fontsize = 20)\n",
    "ax2.set_xlabel('True Force (N)', fontsize = 20)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "# Set min and max range for axes\n",
    "alldata = np.concatenate((np.array(Y_true), Y_pred), axis=0)\n",
    "max_range = roundup(alldata.max())\n",
    "min_range = rounddown(alldata.min())\n",
    "ax2.set_xlim(min_range, max_range)\n",
    "ax2.set_ylim(min_range, max_range)\n",
    "ax2.set_xticks([min_range, max_range])\n",
    "ax2.set_yticks([min_range, max_range])\n",
    "ax2.tick_params(axis='both', which = 'major', labelsize = 16)\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], '--k', linewidth=5, transform=ax2.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot predictions of Z\n",
    "ax3.scatter(Z_true, Z_pred, s=50, alpha=0.5, edgecolors='k', color = 'blue')\n",
    "ax3.set_title('Ulnar (+)', fontsize = 20)\n",
    "ax3.set_xlabel('True Force (N)', fontsize = 20)\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "# Set min and max range for axes\n",
    "alldata = np.concatenate((np.array(Z_true), Z_pred), axis=0)\n",
    "max_range = roundup(alldata.max())\n",
    "min_range = rounddown(alldata.min())\n",
    "ax3.set_xlim(min_range, max_range)\n",
    "ax3.set_ylim(min_range, max_range)\n",
    "ax3.set_xticks([min_range, max_range])\n",
    "ax3.set_yticks([min_range, max_range])\n",
    "ax3.tick_params(axis='both', which = 'major', labelsize = 16)\n",
    "\n",
    "ax3.plot([0, 1], [0, 1], '--k', linewidth=5, transform=ax3.transAxes)\n",
    "plt.savefig(figname, dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample of the data\n",
    "\n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "\n",
    "n = np.random.randint(0, len(test_data.X))\n",
    "print(len(test_data.X))\n",
    "x_plot = test_data.X.reshape(-1, 8).float().detach().cpu().numpy()\n",
    "y_plot = test_data.Y.reshape(-1, 3).float().detach().cpu().numpy()\n",
    "dt = test_data.df['Time'].iloc[1] - test_data.df['Time'].iloc[0]\n",
    "time = np.arange(0, x_plot.shape[0]*dt, dt)\n",
    "print(y_plot.shape)\n",
    "y_labels = test_data.ycols\n",
    "x_labels = test_data.xcols\n",
    "# lets remake this plot but make the two subplots share the x axis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,8), sharex=True)  \n",
    "# make sure the colors for xplot do not overlap with yplot. Make xplot from the viridis colormap. there are 8 columns in xplot\n",
    "for i in range(x_plot.shape[1]):\n",
    "    ax[0].plot(time, x_plot[:,i], color=plt.cm.tab20b(i/x_plot.shape[1]))\n",
    "ax[0].set_ylabel('Muscle Activation')    \n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "for i in range(y_plot.shape[1]):\n",
    "    ax[1].plot(time, y_plot[:,i], color=line_colors[i])\n",
    "ax[1].set_xlabel('Time [s]')\n",
    "ax[1].set_ylabel('Force [N]')\n",
    "ax[1].legend(y_labels)\n",
    "#plot the prediction of this sample \n",
    "pred = tlm(test_data.X.to('cuda')).detach().cpu().numpy().reshape(-1, 3)\n",
    "print(pred.shape)\n",
    "for i in range(pred.shape[1]):\n",
    "    ax[1].plot(time, pred[:,i], color=line_colors[i], linestyle='--')\n",
    "# print the mean prediction on the plot\n",
    "pred_mean = np.mean(pred, axis=0)\n",
    "print(pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample of the data\n",
    "\n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "\n",
    "n = np.random.randint(0, len(test_data.X))\n",
    "print(len(test_data.X))\n",
    "x_plot = test_data.X.reshape(-1, 8).float().detach().cpu().numpy()\n",
    "y_plot = test_data.Y.reshape(-1, 3).float().detach().cpu().numpy()\n",
    "dt = test_data.df['Time'].iloc[1] - test_data.df['Time'].iloc[0]\n",
    "time = np.arange(0, x_plot.shape[0]*dt, dt)\n",
    "print(y_plot.shape)\n",
    "y_labels = test_data.ycols\n",
    "x_labels = test_data.xcols\n",
    "# lets remake this plot but make the two subplots share the x axis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,8), sharex=True)  \n",
    "# make sure the colors for xplot do not overlap with yplot. Make xplot from the viridis colormap. there are 8 columns in xplot\n",
    "for i in range(x_plot.shape[1]):\n",
    "    ax[0].plot(time, x_plot[:,i], color=plt.cm.tab20b(i/x_plot.shape[1]))\n",
    "ax[0].set_ylabel('Muscle Activation')    \n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "for i in range(y_plot.shape[1]):\n",
    "    ax[1].plot(time, y_plot[:,i], color=line_colors[i])\n",
    "ax[1].set_xlabel('Time [s]')\n",
    "ax[1].set_ylabel('Force [N]')\n",
    "ax[1].legend(y_labels)\n",
    "#plot the prediction of this sample \n",
    "pred = tlm(test_data.X.to('cuda')).detach().cpu().numpy().reshape(-1, 3)\n",
    "print(pred.shape)\n",
    "for i in range(pred.shape[1]):\n",
    "    ax[1].plot(time, pred[:,i], color=line_colors[i], linestyle='--')\n",
    "# print the mean prediction on the plot\n",
    "pred_mean = np.mean(pred, axis=0)\n",
    "print(pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample of the data\n",
    "\n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "\n",
    "n = np.random.randint(0, len(train_data.X))\n",
    "print(len(train_data.X))\n",
    "x_plot = np.abs(train_data.X.reshape(-1, 8).float().detach().cpu().numpy())\n",
    "y_plot = np.abs(train_data.Y.reshape(-1, 3).float().detach().cpu().numpy())\n",
    "dt = train_data.df['Time'].iloc[1] - train_data.df['Time'].iloc[0]\n",
    "time = np.arange(0, x_plot.shape[0]*dt, dt)\n",
    "print(y_plot.shape)\n",
    "y_labels = train_data.ycols\n",
    "x_labels = train_data.xcols\n",
    "# lets remake this plot but make the two subplots share the x axis\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15,8), sharex=True)  \n",
    "# make sure the colors for xplot do not overlap with yplot. Make xplot from the viridis colormap. there are 8 columns in xplot\n",
    "for i in range(x_plot.shape[1]):\n",
    "    ax[0].plot(time, x_plot[:,i], color=plt.cm.tab20b(i/x_plot.shape[1]))\n",
    "ax[0].set_ylabel('Muscle Activation')    \n",
    "# line_colors = red, green, blue\n",
    "line_colors = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "for i in range(y_plot.shape[1]):\n",
    "    ax[1].plot(time, y_plot[:,i], color=line_colors[i])\n",
    "ax[1].set_xlabel('Time [s]')\n",
    "ax[1].set_ylabel('Force [N]')\n",
    "ax[1].legend(y_labels)\n",
    "#plot the prediction of this sample \n",
    "pred = np.abs(tlm(train_data.X.to('cuda')).detach().cpu().numpy().reshape(-1, 3))\n",
    "print(pred.shape)\n",
    "for i in range(pred.shape[1]):\n",
    "    ax[1].plot(time, pred[:,i], color=line_colors[i], linestyle='--')\n",
    "# print the mean prediction on the plot\n",
    "pred_mean = np.mean(pred, axis=0)\n",
    "print(pred_mean)\n",
    "#ax[1].set_yscale('log')\n",
    "ax[1].set_xlim([0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_Pytorch(model, X_train, Y_train, n_epochs=100, learning_rate=1e-5, batch_size=int(1e5), device='cuda', optimizer=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    losses = train_pytorch(model, \n",
    "                 X_train, \n",
    "                 Y_train,\n",
    "                 n_epochs=n_epochs,\n",
    "                 batch_size=batch_size, \n",
    "                 learning_rate=learning_rate)\n",
    "    return losses\n",
    "\n",
    "def run_epochs(model, X_train, Y_train, loss_func, optimizer, batches, n_epochs=100, device='cuda'):\n",
    "    t1 = time.time()\n",
    "    losses = []\n",
    "    loss_func = loss_func.to(device)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in batches:\n",
    "           # i = indicies[i]\n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            x = X_train[i,:].to(device)\n",
    "            y = Y_train[i,:].to(device).flatten()\n",
    "            pred = model(x).flatten()\n",
    "            # check if y and pred are the same shape\n",
    "            # if y.shape != pred.shape:\n",
    "            #     print('y and pred are not the same shape')\n",
    "            #     print(y.shape, pred.shape)\n",
    "            #     break\n",
    "            # if x.device != y.device:\n",
    "            #     print('x and y are not on the same device')\n",
    "            #     print(x.device, y.device)\n",
    "            #     break\n",
    "            # if model.device != x.device:\n",
    "            #     print('model and x are not on the same device')\n",
    "            #    print(model.device, x.device)\n",
    "            #    break\n",
    "            loss = loss_func(pred, y) # must be (1. nn output, 2. target)\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "        losses.append(loss)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if epoch%10 == 0:\n",
    "            t2 = time.time()\n",
    "            print('EPOCH : ', epoch,', dt: ',\n",
    "                  t2 - t1, 'seconds, losses :', \n",
    "                  float(loss.detach().cpu())) \n",
    "            t1 = time.time()\n",
    "    return losses\n",
    "\n",
    "def train_pytorch(model, X_train, Y_train, n_epochs=1000, batch_size=int(1e3), learning_rate=1e-3, device='cuda', optimizer=None):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    losses = []\n",
    "    batches = batch_data(X_train, batch_size)\n",
    "    model = model.to(device)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    losses = run_epochs(model, X_train, Y_train, loss_func, optimizer, batches, n_epochs=n_epochs)\n",
    "    return [i.detach().cpu() for i in losses]\n",
    "\n",
    "def batch_data(Y, batch_size):\n",
    "    batch_size = int(batch_size)\n",
    "    n_observations = int(Y.shape[0])\n",
    "    batch_index = np.arange(0, n_observations, batch_size)\n",
    "    #np.random.shuffle(batch_index)\n",
    "    batches = np.array([np.arange(batch_index[i], batch_index[i+1]) \\\n",
    "                   for i in range(len(batch_index)-1)])\n",
    "    shape = batches.shape\n",
    "    temp = batches.reshape(-1,1)\n",
    "    np.random.shuffle(temp)\n",
    "    batches = temp.reshape(shape[0], shape[1])\n",
    "    np.random.shuffle(batches)\n",
    "    n_batches = len(batches)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(nn.Module):\n",
    "    def __init__(self, n_inputs=8, hidden_size=30, n_outputs=300, n_linear_layers=1, \n",
    "                 layer_size=10, lstm_n_outputs=30):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_inputs, hidden_size, batch_first=True)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(layer_size, layer_size) for i in range(n_linear_layers)])\n",
    "        self.layer_size = layer_size\n",
    "        self.n_linear_layers = n_linear_layers \n",
    "        self.lstm_n_outputs = lstm_n_outputs\n",
    "        self.output = nn.Linear(layer_size, n_outputs)\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rows = x.shape[0]\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.reshape(rows, -1)\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            if fc == self.fcs[0]:\n",
    "                if x.shape[1] != int(fc.in_features):\n",
    "                    try:    \n",
    "                        self.fcs[0] = nn.Linear(x.shape[1], self.layer_size)\n",
    "                        x = F.relu(self.fcs[0](x))\n",
    "                    except:\n",
    "                        self.fcs[0] = nn.Linear(x.shape[1], self.layer_size).to(self.device)\n",
    "                        x = F.relu(self.fcs[0](x))\n",
    "                else:\n",
    "                    x = F.relu(fc(x))\n",
    "            else:\n",
    "                x = F.relu(fc(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        self.lstm = self.lstm.to(*args, **kwargs)\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            self.fcs[i] = fc.to(*args, **kwargs)\n",
    "        self.output = self.output.to(*args, **kwargs)\n",
    "        self.device =torch.device(*args, **kwargs)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM3(nn.Module):\n",
    "    def __init__(self, input_size = 8, hidden_size = 128, output_size = 100, num_layers = 4, layer_size = 128):\n",
    "        super(LSTM3, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_size = layer_size\n",
    "        # we will use 3 lstms from kmk.lstm \n",
    "        self.lstms = nn.ModuleList([lstm(n_inputs=input_size, hidden_size=hidden_size, n_outputs=output_size,\n",
    "                                         layer_size=layer_size) for i in range(3)])\n",
    "        self.device = 'cpu'\n",
    "        # x, y, z forces being predicted by each model independantly\n",
    "    def forward(self, x):\n",
    "        # x is a tensor of shape (batch_size, seq_len, input_size)\n",
    "        # we will run each lstm on the same input\n",
    "        prediction = torch.zeros((x.shape[0], x.shape[1], 3)).to(self.device)\n",
    "        for i in range(3):\n",
    "            prediction[:, :, i] = self.lstms[i](x)\n",
    "        prediction = prediction.reshape(-1, 3*x.shape[1])\n",
    "        return prediction\n",
    "    # create a new \"to\" method to ensure all the models stored on this model get transferred to the same device\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        for i in range(3):\n",
    "            self.lstms[i].to(device)\n",
    "            self.lstms.device = device\n",
    "        self.device = torch.device(device)\n",
    "        return self\n",
    "    \n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LSTM3(input_size=9, layer_size=2, num_layers=5)\n",
    "l = l.to('cuda:0')\n",
    "#l = lstm(n_inputs=8, layer_size=2, n_outputs=300, n_linear_layers=20)\n",
    "#train_data.X.shape[0]*train_data.X.shape[1], n_params(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM3(\n",
      "  (lstms): ModuleList(\n",
      "    (0): lstm(\n",
      "      (lstm): LSTM(9, 128, batch_first=True)\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "      )\n",
      "      (output): Linear(in_features=2, out_features=100, bias=True)\n",
      "    )\n",
      "    (1): lstm(\n",
      "      (lstm): LSTM(9, 128, batch_first=True)\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "      )\n",
      "      (output): Linear(in_features=2, out_features=100, bias=True)\n",
      "    )\n",
      "    (2): lstm(\n",
      "      (lstm): LSTM(9, 128, batch_first=True)\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "      )\n",
      "      (output): Linear(in_features=2, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "EPOCH :  0 , dt:  1.904541254043579 seconds, losses : 1108.147216796875\n",
      "EPOCH :  10 , dt:  3.4705891609191895 seconds, losses : 1064.943115234375\n",
      "EPOCH :  20 , dt:  3.4917709827423096 seconds, losses : 1014.2593994140625\n",
      "EPOCH :  30 , dt:  3.411687135696411 seconds, losses : 949.6900024414062\n",
      "EPOCH :  40 , dt:  3.4393937587738037 seconds, losses : 886.896484375\n",
      "EPOCH :  50 , dt:  3.283961057662964 seconds, losses : 825.7996826171875\n",
      "EPOCH :  60 , dt:  3.419576406478882 seconds, losses : 763.169189453125\n",
      "EPOCH :  70 , dt:  3.413303852081299 seconds, losses : 707.9069213867188\n",
      "EPOCH :  80 , dt:  3.3887665271759033 seconds, losses : 659.7001342773438\n",
      "EPOCH :  90 , dt:  3.327670097351074 seconds, losses : 604.2177124023438\n",
      "EPOCH :  100 , dt:  3.394390344619751 seconds, losses : 557.010986328125\n",
      "EPOCH :  110 , dt:  3.3910293579101562 seconds, losses : 507.5299072265625\n",
      "EPOCH :  120 , dt:  3.339332342147827 seconds, losses : 463.6007385253906\n",
      "EPOCH :  130 , dt:  3.321810483932495 seconds, losses : 433.928955078125\n",
      "EPOCH :  140 , dt:  3.4593939781188965 seconds, losses : 384.612548828125\n",
      "EPOCH :  150 , dt:  3.4102296829223633 seconds, losses : 352.616455078125\n",
      "EPOCH :  160 , dt:  3.4654321670532227 seconds, losses : 322.5321044921875\n",
      "EPOCH :  170 , dt:  3.4413695335388184 seconds, losses : 296.4098815917969\n",
      "EPOCH :  180 , dt:  3.343471050262451 seconds, losses : 271.50323486328125\n",
      "EPOCH :  190 , dt:  3.4065487384796143 seconds, losses : 251.65023803710938\n",
      "EPOCH :  200 , dt:  3.388843059539795 seconds, losses : 231.87374877929688\n",
      "EPOCH :  210 , dt:  3.3509840965270996 seconds, losses : 223.80555725097656\n",
      "EPOCH :  220 , dt:  3.455655574798584 seconds, losses : 207.10279846191406\n",
      "EPOCH :  230 , dt:  3.2761716842651367 seconds, losses : 193.42449951171875\n",
      "EPOCH :  240 , dt:  3.3738527297973633 seconds, losses : 192.2783660888672\n",
      "EPOCH :  250 , dt:  3.461698055267334 seconds, losses : 180.93531799316406\n",
      "EPOCH :  260 , dt:  3.3391642570495605 seconds, losses : 171.4180908203125\n",
      "EPOCH :  270 , dt:  3.5999574661254883 seconds, losses : 172.24807739257812\n",
      "EPOCH :  280 , dt:  3.4273793697357178 seconds, losses : 167.10658264160156\n",
      "EPOCH :  290 , dt:  3.38427996635437 seconds, losses : 158.88372802734375\n",
      "EPOCH :  300 , dt:  3.3255600929260254 seconds, losses : 164.75389099121094\n",
      "EPOCH :  310 , dt:  3.4395713806152344 seconds, losses : 147.95062255859375\n",
      "EPOCH :  320 , dt:  4.315634489059448 seconds, losses : 153.84226989746094\n",
      "EPOCH :  330 , dt:  4.278951168060303 seconds, losses : 128.56614685058594\n",
      "EPOCH :  340 , dt:  4.288167476654053 seconds, losses : 141.03932189941406\n",
      "EPOCH :  350 , dt:  4.300243139266968 seconds, losses : 143.589111328125\n",
      "EPOCH :  360 , dt:  4.186585426330566 seconds, losses : 121.98534393310547\n",
      "EPOCH :  370 , dt:  4.183576345443726 seconds, losses : 118.50955963134766\n",
      "EPOCH :  380 , dt:  4.374291658401489 seconds, losses : 120.77375030517578\n",
      "EPOCH :  390 , dt:  4.443761110305786 seconds, losses : 113.96465301513672\n",
      "EPOCH :  400 , dt:  4.301933765411377 seconds, losses : 114.2021484375\n",
      "EPOCH :  410 , dt:  4.122994661331177 seconds, losses : 115.21332550048828\n",
      "EPOCH :  420 , dt:  4.273930072784424 seconds, losses : 114.06922149658203\n",
      "EPOCH :  430 , dt:  4.211750030517578 seconds, losses : 118.66703033447266\n",
      "EPOCH :  440 , dt:  3.6870675086975098 seconds, losses : 115.43416595458984\n",
      "EPOCH :  450 , dt:  4.08770751953125 seconds, losses : 115.76876831054688\n",
      "EPOCH :  460 , dt:  3.8316876888275146 seconds, losses : 115.17759704589844\n",
      "EPOCH :  470 , dt:  4.137039661407471 seconds, losses : 114.65221405029297\n",
      "EPOCH :  480 , dt:  3.7935917377471924 seconds, losses : 113.3271255493164\n",
      "EPOCH :  490 , dt:  4.1191606521606445 seconds, losses : 114.95409393310547\n",
      "EPOCH :  500 , dt:  4.5311667919158936 seconds, losses : 115.21837615966797\n",
      "EPOCH :  510 , dt:  4.1928136348724365 seconds, losses : 120.27617645263672\n",
      "EPOCH :  520 , dt:  4.4064555168151855 seconds, losses : 113.81897735595703\n",
      "EPOCH :  530 , dt:  4.785103797912598 seconds, losses : 117.36580657958984\n",
      "EPOCH :  540 , dt:  4.718839168548584 seconds, losses : 108.1600112915039\n",
      "EPOCH :  550 , dt:  4.3887269496917725 seconds, losses : 110.2892837524414\n",
      "EPOCH :  560 , dt:  3.9910993576049805 seconds, losses : 108.15992736816406\n",
      "EPOCH :  570 , dt:  4.612626791000366 seconds, losses : 108.93436431884766\n",
      "EPOCH :  580 , dt:  4.647186756134033 seconds, losses : 105.17339324951172\n",
      "EPOCH :  590 , dt:  4.140833854675293 seconds, losses : 105.52676391601562\n",
      "EPOCH :  600 , dt:  3.711839199066162 seconds, losses : 104.00232696533203\n",
      "EPOCH :  610 , dt:  3.8969717025756836 seconds, losses : 100.6934814453125\n",
      "EPOCH :  620 , dt:  4.391139030456543 seconds, losses : 94.09538269042969\n",
      "EPOCH :  630 , dt:  4.32388973236084 seconds, losses : 96.42333221435547\n",
      "EPOCH :  640 , dt:  4.497931957244873 seconds, losses : 94.74266052246094\n",
      "EPOCH :  650 , dt:  4.441275119781494 seconds, losses : 92.14751434326172\n",
      "EPOCH :  660 , dt:  3.7699174880981445 seconds, losses : 94.42351531982422\n",
      "EPOCH :  670 , dt:  3.873021125793457 seconds, losses : 95.87493896484375\n",
      "EPOCH :  680 , dt:  4.151217222213745 seconds, losses : 91.95183563232422\n",
      "EPOCH :  690 , dt:  4.505539894104004 seconds, losses : 89.6316909790039\n",
      "EPOCH :  700 , dt:  4.421448707580566 seconds, losses : 87.50999450683594\n",
      "EPOCH :  710 , dt:  4.545869827270508 seconds, losses : 92.36837768554688\n",
      "EPOCH :  720 , dt:  4.425870895385742 seconds, losses : 93.62936401367188\n",
      "EPOCH :  730 , dt:  4.477057218551636 seconds, losses : 88.02439880371094\n",
      "EPOCH :  740 , dt:  4.372063636779785 seconds, losses : 87.27899932861328\n",
      "EPOCH :  750 , dt:  4.49790358543396 seconds, losses : 87.29853057861328\n",
      "EPOCH :  760 , dt:  4.43307638168335 seconds, losses : 104.07154846191406\n",
      "EPOCH :  770 , dt:  4.397861957550049 seconds, losses : 96.20458984375\n",
      "EPOCH :  780 , dt:  4.587719917297363 seconds, losses : 85.25313568115234\n",
      "EPOCH :  790 , dt:  4.3492724895477295 seconds, losses : 91.19217681884766\n",
      "EPOCH :  800 , dt:  4.522767782211304 seconds, losses : 93.5648422241211\n",
      "EPOCH :  810 , dt:  4.577194690704346 seconds, losses : 92.49444580078125\n",
      "EPOCH :  820 , dt:  4.285663604736328 seconds, losses : 84.2667465209961\n",
      "EPOCH :  830 , dt:  4.314146041870117 seconds, losses : 84.75873565673828\n",
      "EPOCH :  840 , dt:  4.650445938110352 seconds, losses : 83.20960998535156\n",
      "EPOCH :  850 , dt:  4.621220350265503 seconds, losses : 86.80072784423828\n",
      "EPOCH :  860 , dt:  4.606708526611328 seconds, losses : 82.27755737304688\n",
      "EPOCH :  870 , dt:  4.069965600967407 seconds, losses : 90.82350158691406\n",
      "EPOCH :  880 , dt:  4.719068765640259 seconds, losses : 92.51316833496094\n",
      "EPOCH :  890 , dt:  4.442379713058472 seconds, losses : 83.95819091796875\n",
      "EPOCH :  900 , dt:  4.629448175430298 seconds, losses : 86.5208740234375\n",
      "EPOCH :  910 , dt:  4.308013916015625 seconds, losses : 80.91654205322266\n",
      "EPOCH :  920 , dt:  4.262388229370117 seconds, losses : 82.61840057373047\n",
      "EPOCH :  930 , dt:  4.580214738845825 seconds, losses : 81.619140625\n",
      "EPOCH :  940 , dt:  4.3863794803619385 seconds, losses : 81.16385650634766\n",
      "EPOCH :  950 , dt:  4.458350419998169 seconds, losses : 82.59629821777344\n",
      "EPOCH :  960 , dt:  4.786116600036621 seconds, losses : 79.07307434082031\n",
      "EPOCH :  970 , dt:  4.762431859970093 seconds, losses : 83.05118560791016\n",
      "EPOCH :  980 , dt:  4.265752553939819 seconds, losses : 79.03962707519531\n",
      "EPOCH :  990 , dt:  4.33933687210083 seconds, losses : 78.27552032470703\n",
      "EPOCH :  1000 , dt:  4.703179836273193 seconds, losses : 76.86686706542969\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X = train_data.X.to('cuda:0')\n",
    "Y = train_data.Y.to('cuda:0')\n",
    "print(l)\n",
    "lr = 1e-3\n",
    "loss = run_Pytorch(l, X, Y,  learning_rate=lr,\n",
    "                    n_epochs=1001, batch_size=4, device=torch.device('cuda:0'), optimizer=torch.optim.Adam(l.parameters(), lr=lr, weight_decay=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to plot a paper worthy learning curve of the model. Make the line stand out in a clean cartoonish way. Make the y axis\n",
    "# log scale. \n",
    "def plot_learning_curve(losses, title='Learning Curve', xlabel='Epoch', ylabel='Loss', figsize=(10,10)):\n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yscale('log')\n",
    "    # add a grid\n",
    "    ax.grid()\n",
    "    return fig, ax\n",
    "plot_learning_curve(loss, title='Learning Curve', xlabel='Epoch', ylabel='Loss', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_data.X.to('cuda:0')\n",
    "Y = np.abs(test_data.Y.to('cpu').reshape(-1, 3)).cpu().numpy()\n",
    "pred = np.abs(l(X).reshape(-1, 3).detach().cpu().numpy())\n",
    "\n",
    "Force_colors = ['r', 'g', 'b']\n",
    "for i in range(3):\n",
    "    plt.plot(Y[:, i], Force_colors[i], label=f'Y_{i}')\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(pred[:, i], Force_colors[i], label=f'pred_{i}', linestyle='--')\n",
    "\n",
    "plt.yscale('log')\n",
    "#pred.shape, Y.shape, Fx.shape, Fy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parity plot\n",
    "plt.scatter(Y[:, 0], pred[:, 0], color=Force_colors[0], label='Fx', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.scatter(Y[:, 1], pred[:, 1], color=Force_colors[1], label='Fy', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.scatter(Y[:, 2], pred[:, 2], color=Force_colors[2], label='Fz', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot magnitude of forces as a parity plot\n",
    "F = np.sqrt(Y[:, 0]**2 + Y[:, 1]**2 + Y[:, 2]**2)\n",
    "F_pred = np.sqrt(pred[:, 0]**2 + pred[:, 1]**2 + pred[:, 2]**2)\n",
    "plt.scatter(F, F_pred, color='k', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linear = train_data.X.reshape(-1, 8).to('cpu').numpy()\n",
    "y_linear = train_data.Y.reshape(-1, 3).to('cpu').numpy()[:,1]\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# reg   = LinearRegression().fit(x_linear, y_linear)\n",
    "# reg = RandomForestRegressor(n_estimators = 100, max_depth=10, random_state=0).fit(x_linear, y_linear)\n",
    "reg = GradientBoostingRegressor(n_estimators=100, max_depth=10, random_state=0).fit(x_linear, y_linear)\n",
    "reg.fit(x_linear, y_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = reg.predict(test_data.X.reshape(-1, 8).to('cpu').numpy())\n",
    "y_plot = test_data.Y.reshape(-1, 3).to('cpu').numpy()[:,1]\n",
    "plt.scatter(y_plot, pred, color=Force_colors[0], label='Fx', edgecolors='k', linewidths=0.5, alpha=0.5)  \n",
    "# plt.scatter(y_plot[:,1], pred[:, 1], color=Force_colors[1], label='Fy', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "# plt.scatter(y_plot[:,2], pred[:, 2], color=Force_colors[2], label='Fz', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='k', linestyle='--')\n",
    "mae = np.mean(np.abs(y_plot - pred))\n",
    "mse = np.mean((y_plot - pred)**2)\n",
    "r2 = 1 - np.sum((y_plot - pred)**2)/np.sum((y_plot - np.mean(y_plot))**2)\n",
    "print('MAE : ', mae)\n",
    "print('MSE : ', mse)\n",
    "print('R2 : ', r2)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_range = [10, 50, 100, 200, 500, 1000]\n",
    "n_depths = [5, 10, 20, 50, 100]\n",
    "leaf_splits = [2, 5, 10, 20, 50, 100]\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "cv_search = RandomizedSearchCV(RandomForestRegressor(), param_distributions={'n_estimators': n_estimators_range, 'max_depth': n_depths, 'min_samples_leaf': leaf_splits}, n_iter=10, n_jobs=-1, cv=5, verbose=3)\n",
    "cv_search.fit(x_linear, y_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(n_estimators=100, max_depth=10, random_state=0).fit(x_linear, y_linear)\n",
    "reg.fit(x_linear, y_linear)\n",
    "mae = np.mean(np.abs(y_plot - pred))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SVR from sklearn\n",
    "from sklearn.svm import SVR\n",
    "reg = SVR().fit(x_linear, y_linear)\n",
    "pred = reg.predict(test_data.X.reshape(-1, 8).to('cpu').numpy())\n",
    "plt.scatter(y_plot, pred, color=Force_colors[0], label='Fx', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='k', linestyle='--')\n",
    "mae = np.mean(np.abs(y_plot - pred))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gaussian process regressor from sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor(hidden_layer_sizes=(8, 8, 8)).fit(x_linear, y_linear)\n",
    "pred = reg.predict(test_data.X.reshape(-1, 8).to('cpu').numpy())\n",
    "plt.scatter(y_plot, pred, color=Force_colors[0], label='Fx', edgecolors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, color='k', linestyle='--')\n",
    "mae = np.mean(np.abs(y_plot - pred))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that given a pytorch model will return the total number of learnable parameters\n",
    "\n",
    "def n_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "l = LSTM3(input_size=8, output_size=100, layer_size=1, num_layers=1)\n",
    "\n",
    "len(l.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c1d3e12e66dc47a0aae61db106152a68be04014ca291cb297471e9102f95b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
