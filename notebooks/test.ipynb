{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\efons\\\\Desktop\\\\kalyn_lstm\\\\kmodels\\\\notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kmodels as kmk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>FCU</th>\n",
       "      <th>EPL</th>\n",
       "      <th>EPB</th>\n",
       "      <th>FPL</th>\n",
       "      <th>APL</th>\n",
       "      <th>FPB</th>\n",
       "      <th>OPP</th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "      <th>Fz</th>\n",
       "      <th>ADD</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054566</td>\n",
       "      <td>0.058537</td>\n",
       "      <td>0.062743</td>\n",
       "      <td>0.155728</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.070465</td>\n",
       "      <td>-9.259692</td>\n",
       "      <td>-4.447818</td>\n",
       "      <td>-6.536988</td>\n",
       "      <td>0.026819</td>\n",
       "      <td>6.26_0.9_1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009799</td>\n",
       "      <td>0.054646</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.062911</td>\n",
       "      <td>0.154414</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-9.502468</td>\n",
       "      <td>-4.600985</td>\n",
       "      <td>-6.520491</td>\n",
       "      <td>0.026349</td>\n",
       "      <td>6.26_0.9_1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019599</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.057753</td>\n",
       "      <td>0.062681</td>\n",
       "      <td>0.153220</td>\n",
       "      <td>0.023763</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>0.095268</td>\n",
       "      <td>-9.513043</td>\n",
       "      <td>-4.624847</td>\n",
       "      <td>-6.518658</td>\n",
       "      <td>0.026434</td>\n",
       "      <td>6.26_0.9_1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029398</td>\n",
       "      <td>0.056122</td>\n",
       "      <td>0.056917</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.157919</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.022113</td>\n",
       "      <td>0.114959</td>\n",
       "      <td>-9.409544</td>\n",
       "      <td>-4.564386</td>\n",
       "      <td>-6.497891</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>6.26_0.9_1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039197</td>\n",
       "      <td>0.057602</td>\n",
       "      <td>0.056416</td>\n",
       "      <td>0.063981</td>\n",
       "      <td>0.163014</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.126942</td>\n",
       "      <td>-9.251525</td>\n",
       "      <td>-4.474644</td>\n",
       "      <td>-6.437751</td>\n",
       "      <td>0.026858</td>\n",
       "      <td>6.26_0.9_1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345171</th>\n",
       "      <td>1.950067</td>\n",
       "      <td>0.077327</td>\n",
       "      <td>0.028705</td>\n",
       "      <td>0.070763</td>\n",
       "      <td>0.186818</td>\n",
       "      <td>0.081797</td>\n",
       "      <td>0.022118</td>\n",
       "      <td>0.090759</td>\n",
       "      <td>-4.916051</td>\n",
       "      <td>-5.716611</td>\n",
       "      <td>-12.003086</td>\n",
       "      <td>0.024750</td>\n",
       "      <td>6.53_1.04_1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345172</th>\n",
       "      <td>1.959866</td>\n",
       "      <td>0.077751</td>\n",
       "      <td>0.027808</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>0.191791</td>\n",
       "      <td>0.082448</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>-5.248973</td>\n",
       "      <td>-5.937422</td>\n",
       "      <td>-12.067884</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>6.53_1.04_1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345173</th>\n",
       "      <td>1.969666</td>\n",
       "      <td>0.078584</td>\n",
       "      <td>0.027024</td>\n",
       "      <td>0.071566</td>\n",
       "      <td>0.193814</td>\n",
       "      <td>0.083229</td>\n",
       "      <td>0.022089</td>\n",
       "      <td>0.093776</td>\n",
       "      <td>-5.556444</td>\n",
       "      <td>-6.157407</td>\n",
       "      <td>-12.143441</td>\n",
       "      <td>0.023570</td>\n",
       "      <td>6.53_1.04_1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345174</th>\n",
       "      <td>1.979465</td>\n",
       "      <td>0.078977</td>\n",
       "      <td>0.026315</td>\n",
       "      <td>0.071994</td>\n",
       "      <td>0.195077</td>\n",
       "      <td>0.083762</td>\n",
       "      <td>0.022077</td>\n",
       "      <td>0.086941</td>\n",
       "      <td>-5.809779</td>\n",
       "      <td>-6.380327</td>\n",
       "      <td>-12.206728</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>6.53_1.04_1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345175</th>\n",
       "      <td>1.989264</td>\n",
       "      <td>0.079077</td>\n",
       "      <td>0.025693</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>0.196594</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.022066</td>\n",
       "      <td>0.077667</td>\n",
       "      <td>-6.071978</td>\n",
       "      <td>-6.645673</td>\n",
       "      <td>-12.258107</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>6.53_1.04_1.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345176 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time       FCU       EPL       EPB       FPL       APL       FPB  \\\n",
       "0        0.000000  0.054566  0.058537  0.062743  0.155728  0.023906  0.021142   \n",
       "1        0.009799  0.054646  0.058400  0.062911  0.154414  0.023787  0.021018   \n",
       "2        0.019599  0.054800  0.057753  0.062681  0.153220  0.023763  0.021220   \n",
       "3        0.029398  0.056122  0.056917  0.063201  0.157919  0.023762  0.022113   \n",
       "4        0.039197  0.057602  0.056416  0.063981  0.163014  0.023785  0.023148   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1345171  1.950067  0.077327  0.028705  0.070763  0.186818  0.081797  0.022118   \n",
       "1345172  1.959866  0.077751  0.027808  0.071065  0.191791  0.082448  0.022103   \n",
       "1345173  1.969666  0.078584  0.027024  0.071566  0.193814  0.083229  0.022089   \n",
       "1345174  1.979465  0.078977  0.026315  0.071994  0.195077  0.083762  0.022077   \n",
       "1345175  1.989264  0.079077  0.025693  0.072354  0.196594  0.084091  0.022066   \n",
       "\n",
       "              OPP        Fx        Fy         Fz       ADD         Subject  \n",
       "0        0.070465 -9.259692 -4.447818  -6.536988  0.026819   6.26_0.9_1.06  \n",
       "1        0.077124 -9.502468 -4.600985  -6.520491  0.026349   6.26_0.9_1.06  \n",
       "2        0.095268 -9.513043 -4.624847  -6.518658  0.026434   6.26_0.9_1.06  \n",
       "3        0.114959 -9.409544 -4.564386  -6.497891  0.026648   6.26_0.9_1.06  \n",
       "4        0.126942 -9.251525 -4.474644  -6.437751  0.026858   6.26_0.9_1.06  \n",
       "...           ...       ...       ...        ...       ...             ...  \n",
       "1345171  0.090759 -4.916051 -5.716611 -12.003086  0.024750  6.53_1.04_1.02  \n",
       "1345172  0.095652 -5.248973 -5.937422 -12.067884  0.024120  6.53_1.04_1.02  \n",
       "1345173  0.093776 -5.556444 -6.157407 -12.143441  0.023570  6.53_1.04_1.02  \n",
       "1345174  0.086941 -5.809779 -6.380327 -12.206728  0.023072  6.53_1.04_1.02  \n",
       "1345175  0.077667 -6.071978 -6.645673 -12.258107  0.022634  6.53_1.04_1.02  \n",
       "\n",
       "[1345176 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../../data/'\n",
    "# get all pickles\n",
    "pickles = [f for f in os.listdir(data_path) if f.endswith('.pkl')]\n",
    "# get the Formatted pickle\n",
    "data = [f for f in pickles if 'Formatted' in f][0]\n",
    "df = pd.read_pickle(data_path + data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'EPB', 'APL', 'FPB', 'FCU', 'FPL', 'EPL', 'OPP']\n",
      "['Fx', 'Fy', 'Fz']\n"
     ]
    }
   ],
   "source": [
    "xcols = df.columns\n",
    "# drop \"ADD\", and \"Subject\"\n",
    "xcols = list(xcols.drop(['ADD', 'Subject']))\n",
    "ycols = ['Fx','Fy','Fz']\n",
    "# drop all the ycols from xcols\n",
    "xcols = list(set(xcols) - set(ycols))\n",
    "print(xcols)\n",
    "print(ycols)\n",
    "# lets normalize all the xcols data and put it back in the dataframe\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(df[xcols])\n",
    "df[xcols] = x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search_results = kmk.random_search(df, batch_size=10000, n_epochs=11, n_iter=10, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_search_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmk.NN(layer_size=50, layers=3, n_inputs=len(xcols), n_outputs=len(ycols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518\n"
     ]
    }
   ],
   "source": [
    "data = kmk.Dataset(df, sort_column=['Subject'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 204, 8]), torch.Size([24, 204, 3]), torch.Size([204, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_df = pd.read_pickle('../../data/SeparatedData.pkl')\n",
    "exp_data = kmk.Dataset(exp_df, sort_column=['Subject'])\n",
    "exp_data.X_lstm.shape, exp_data.Y_lstm.shape, exp_data.Y_lstm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\efons\\AppData\\Local\\Temp\\ipykernel_12872\\1156456482.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test = torch.tensor(exp_data.X_lstm[0:1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.0565e-02, -6.7544e-02, -3.7853e-02],\n",
       "        [-7.6238e-02, -1.1457e-01, -1.4739e-01],\n",
       "        [-6.8136e-02, -8.3564e-02, -1.0196e-01],\n",
       "        [-2.5904e-02,  6.5443e-02, -1.5824e-01],\n",
       "        [-1.5480e-01,  1.5132e-02, -8.7733e-02],\n",
       "        [-8.8710e-02, -3.4445e-02,  2.3464e-02],\n",
       "        [-1.0053e-01, -3.2440e-02, -6.1174e-02],\n",
       "        [ 7.5752e-03, -1.2479e-01,  9.2471e-02],\n",
       "        [-1.5897e-01,  1.1926e-01, -1.1268e-01],\n",
       "        [-1.8198e-01,  5.3528e-02, -1.2466e-01],\n",
       "        [-1.3076e-01, -9.0927e-02,  3.7879e-02],\n",
       "        [ 5.7908e-02, -1.4597e-02, -1.1608e-01],\n",
       "        [ 2.4974e-02, -8.4321e-02, -1.3568e-01],\n",
       "        [-8.1687e-02,  6.7012e-02,  1.1693e-01],\n",
       "        [ 2.6076e-01, -1.0606e-01,  1.0829e-01],\n",
       "        [ 3.1080e-02, -2.8567e-01,  1.1045e-02],\n",
       "        [-1.2413e-01, -1.0555e-01, -2.0011e-02],\n",
       "        [ 1.4949e-01,  5.6415e-04, -1.4077e-01],\n",
       "        [-4.6370e-02, -7.1377e-03,  8.7054e-02],\n",
       "        [ 1.6137e-02, -1.5606e-01, -1.0999e-02],\n",
       "        [-1.0742e-02, -5.2750e-02, -1.4510e-01],\n",
       "        [ 9.2179e-02,  8.4436e-02, -1.5496e-01],\n",
       "        [ 2.1016e-02, -7.9529e-02, -7.3396e-02],\n",
       "        [ 3.2688e-02, -1.5977e-01, -1.6448e-01],\n",
       "        [ 1.0003e-01,  1.5147e-01,  1.1545e-01],\n",
       "        [-5.7700e-02, -1.2350e-01,  7.9035e-02],\n",
       "        [-4.4340e-03, -1.6097e-01,  5.5250e-02],\n",
       "        [ 5.7660e-02,  8.4723e-02,  9.8655e-02],\n",
       "        [ 4.7306e-02, -1.9758e-01,  9.8793e-02],\n",
       "        [ 4.4027e-02, -1.4267e-02, -4.2856e-03],\n",
       "        [ 5.8518e-02,  1.4592e-01,  2.5699e-02],\n",
       "        [-5.8832e-02, -5.9056e-02,  5.8498e-02],\n",
       "        [ 7.6228e-03, -5.8556e-02, -4.4662e-02],\n",
       "        [ 1.1729e-01, -4.8577e-02, -1.2550e-01],\n",
       "        [ 7.4365e-03, -1.1655e-01, -1.3095e-01],\n",
       "        [ 1.4981e-02,  8.6093e-02,  5.1229e-02],\n",
       "        [-1.4362e-01, -6.7628e-03,  2.4608e-02],\n",
       "        [-1.0103e-01,  8.8097e-02, -1.2363e-01],\n",
       "        [ 1.2331e-01, -5.9631e-02, -1.4508e-01],\n",
       "        [ 9.8026e-02, -1.4772e-01,  2.1190e-02],\n",
       "        [ 9.5223e-02, -1.1048e-01, -5.3081e-02],\n",
       "        [ 2.0358e-01, -1.2994e-02,  7.3600e-02],\n",
       "        [-1.6833e-02,  1.5082e-01, -7.9601e-02],\n",
       "        [ 1.4250e-01, -2.1539e-01,  8.4316e-02],\n",
       "        [-1.6600e-01, -1.6151e-01,  7.3994e-02],\n",
       "        [ 1.5597e-01, -5.5054e-02,  1.2526e-02],\n",
       "        [-2.3046e-01,  7.5897e-02,  2.8708e-02],\n",
       "        [-1.0778e-01,  1.6858e-01, -1.1976e-01],\n",
       "        [-1.0747e-01,  6.1151e-02,  4.8992e-02],\n",
       "        [-2.2726e-02, -2.3638e-02,  6.0636e-02],\n",
       "        [ 4.8590e-02, -1.4407e-01,  9.0104e-02],\n",
       "        [ 4.2894e-02,  5.2121e-02,  1.1918e-01],\n",
       "        [ 5.4102e-02, -1.0585e-01, -6.8378e-02],\n",
       "        [-9.1401e-03, -7.3346e-03, -5.2531e-02],\n",
       "        [-2.0248e-02,  4.2321e-02, -1.8436e-01],\n",
       "        [-1.2506e-01,  8.9787e-02,  1.8098e-01],\n",
       "        [-4.7793e-02, -2.3325e-02, -1.8505e-02],\n",
       "        [-3.6008e-02,  1.4499e-01, -1.9493e-01],\n",
       "        [-1.7563e-01, -9.8466e-02, -1.0411e-01],\n",
       "        [-5.2842e-02,  4.2778e-03, -5.5142e-02],\n",
       "        [ 1.4501e-01, -1.0117e-01, -9.2019e-02],\n",
       "        [ 5.6404e-02,  1.2538e-01,  2.6257e-02],\n",
       "        [-1.6756e-01, -1.5629e-02, -1.4898e-01],\n",
       "        [-9.9876e-02,  8.7312e-02,  6.7087e-02],\n",
       "        [ 8.0276e-02,  6.0289e-02,  1.0721e-01],\n",
       "        [ 5.5661e-02,  4.8748e-02, -6.0810e-02],\n",
       "        [ 1.2351e-01,  1.5264e-01,  1.8802e-02],\n",
       "        [-6.0633e-02,  6.3775e-02,  1.1275e-01],\n",
       "        [ 4.7807e-02,  1.5312e-01, -1.1964e-01],\n",
       "        [-7.6852e-02, -3.6594e-02, -5.3642e-02],\n",
       "        [-1.0188e-01, -4.8715e-02, -3.2622e-01],\n",
       "        [-1.1964e-01,  1.5881e-01,  4.0461e-02],\n",
       "        [ 7.2435e-02, -1.6123e-01, -6.2871e-02],\n",
       "        [ 7.4999e-02,  2.0107e-02, -5.7209e-02],\n",
       "        [-1.4196e-01, -5.8712e-02, -1.3023e-02],\n",
       "        [ 5.0048e-02,  2.0792e-01, -1.6273e-01],\n",
       "        [ 1.8098e-01, -7.9229e-02, -1.2339e-01],\n",
       "        [-2.2717e-02, -2.7113e-02,  5.2083e-02],\n",
       "        [-8.8856e-02,  1.8935e-02, -1.3183e-01],\n",
       "        [ 1.3003e-01,  9.4909e-02, -5.8380e-03],\n",
       "        [-1.1628e-01, -1.1298e-01, -2.0927e-01],\n",
       "        [ 6.6325e-03, -3.8402e-03, -1.6303e-01],\n",
       "        [ 9.1677e-03, -1.0687e-01, -7.4677e-03],\n",
       "        [-1.0011e-01, -3.6438e-02,  4.3123e-02],\n",
       "        [ 1.7992e-01,  1.3649e-01,  6.7965e-02],\n",
       "        [-1.6716e-01, -1.2969e-01,  8.3648e-03],\n",
       "        [ 5.8624e-02, -8.7065e-02,  1.5663e-01],\n",
       "        [-6.2315e-02,  2.3316e-02,  7.7659e-02],\n",
       "        [ 1.4572e-01,  9.6795e-02, -1.1243e-01],\n",
       "        [-9.7320e-02,  6.5512e-04,  1.0316e-02],\n",
       "        [-1.3745e-01, -5.5586e-02, -1.3713e-02],\n",
       "        [-2.6621e-02,  6.5049e-02, -1.7429e-01],\n",
       "        [-4.9439e-02, -3.0280e-02,  1.3080e-01],\n",
       "        [ 2.9868e-02, -1.5323e-01, -1.1829e-01],\n",
       "        [-1.4689e-01,  5.9860e-02, -7.6183e-03],\n",
       "        [ 1.4487e-01,  2.5847e-01, -1.4023e-01],\n",
       "        [ 7.4443e-02,  5.0301e-02, -1.5701e-01],\n",
       "        [ 5.4939e-03, -5.4064e-02,  7.7951e-02],\n",
       "        [ 3.5203e-02, -9.6350e-02,  5.3278e-03],\n",
       "        [ 5.2614e-04,  6.8824e-02,  4.7911e-02],\n",
       "        [ 4.8449e-02, -8.3335e-02, -2.0579e-02],\n",
       "        [-2.4332e-02, -1.8664e-01,  3.0555e-02],\n",
       "        [ 9.9954e-02,  1.7525e-01,  8.7581e-02],\n",
       "        [-5.0343e-02,  7.0242e-02, -1.1969e-01],\n",
       "        [ 3.9634e-02, -9.8048e-04, -5.3979e-02],\n",
       "        [-1.7952e-01, -1.9327e-02,  9.9320e-02],\n",
       "        [ 7.5807e-02,  2.5439e-02,  3.5633e-02],\n",
       "        [ 1.5342e-01, -7.8589e-03, -4.9672e-02],\n",
       "        [ 4.9675e-02,  1.4552e-02, -5.9570e-02],\n",
       "        [ 4.5777e-02,  3.5792e-02,  3.2383e-02],\n",
       "        [-4.8336e-02,  1.8686e-01,  1.0066e-01],\n",
       "        [ 1.6688e-01,  1.0545e-01,  2.1284e-01],\n",
       "        [ 7.1210e-02,  7.1522e-02,  2.6011e-02],\n",
       "        [-1.1146e-01, -9.2252e-02, -1.0830e-01],\n",
       "        [ 9.3452e-02,  6.4215e-02, -1.3266e-01],\n",
       "        [ 1.0586e-01, -1.2762e-01, -4.9333e-02],\n",
       "        [-8.8146e-03,  3.9832e-02, -8.6628e-02],\n",
       "        [-2.1035e-01,  8.9866e-02, -7.9615e-02],\n",
       "        [-1.2035e-01,  1.6320e-02,  5.0436e-02],\n",
       "        [ 5.8973e-02,  2.6070e-02,  1.2362e-01],\n",
       "        [-2.4192e-01, -1.2951e-01, -1.6041e-01],\n",
       "        [ 5.2200e-03, -9.4753e-03, -1.6211e-02],\n",
       "        [ 1.4929e-01, -1.6730e-01,  9.5905e-03],\n",
       "        [-2.9548e-02, -2.2253e-01,  1.6244e-01],\n",
       "        [-2.3928e-01, -6.1366e-03,  2.8390e-02],\n",
       "        [ 2.9408e-02,  9.3836e-02,  4.4921e-02],\n",
       "        [ 1.1398e-01, -1.1914e-01,  1.1306e-02],\n",
       "        [-1.2255e-01, -1.1038e-01, -5.8955e-03],\n",
       "        [-2.8340e-02,  5.3664e-02, -2.5777e-01],\n",
       "        [-8.5744e-02,  1.0625e-01, -1.2983e-01],\n",
       "        [-7.2804e-03,  7.2747e-02, -3.7183e-02],\n",
       "        [ 3.3449e-02,  1.6728e-01, -1.0149e-01],\n",
       "        [-1.6549e-01, -3.8955e-02,  1.5212e-02],\n",
       "        [ 2.4892e-02, -5.3863e-02, -1.1389e-01],\n",
       "        [-3.2976e-02, -1.7722e-01,  2.9308e-02],\n",
       "        [ 3.2441e-02,  7.0600e-03,  1.0576e-01],\n",
       "        [ 4.6327e-02, -4.7071e-02, -4.1734e-02],\n",
       "        [-3.0672e-02,  7.0332e-02, -8.0280e-02],\n",
       "        [ 6.9734e-02, -2.1746e-02,  7.0104e-02],\n",
       "        [-1.3728e-01,  1.3359e-01, -1.0388e-01],\n",
       "        [ 5.6644e-03, -1.1355e-01,  1.4169e-01],\n",
       "        [-3.1021e-02,  1.6625e-01, -9.8078e-02],\n",
       "        [ 6.2865e-02, -1.0615e-01,  1.3091e-01],\n",
       "        [-6.2561e-02, -1.7124e-01, -2.2941e-01],\n",
       "        [ 1.0938e-01, -5.5912e-03, -6.0572e-02],\n",
       "        [-8.7822e-02, -4.9075e-02, -1.3502e-01],\n",
       "        [-1.4892e-01, -6.1183e-03,  2.1393e-02],\n",
       "        [-1.9139e-02,  1.3892e-01, -3.0516e-02],\n",
       "        [ 6.4493e-02, -7.1295e-02,  7.6139e-02],\n",
       "        [-8.6745e-02,  3.6232e-02,  4.9644e-02],\n",
       "        [ 6.0360e-02,  1.7739e-01, -3.9480e-02],\n",
       "        [ 1.7926e-02,  2.3077e-02,  2.4475e-02],\n",
       "        [ 1.3457e-04,  1.1883e-01, -7.0426e-02],\n",
       "        [ 6.8035e-02, -1.4771e-01,  1.3744e-01],\n",
       "        [-4.3948e-02, -1.1330e-01, -6.4809e-02],\n",
       "        [ 5.6365e-02,  1.3665e-01, -9.0599e-02],\n",
       "        [ 3.9358e-02, -1.3228e-01,  2.1322e-02],\n",
       "        [ 1.3131e-01, -1.2896e-01, -8.1712e-02],\n",
       "        [-5.7581e-02,  1.6751e-01,  1.0091e-01],\n",
       "        [ 5.8135e-02, -1.6366e-01,  1.0578e-01],\n",
       "        [ 1.2990e-01, -4.0629e-02, -5.5265e-02],\n",
       "        [ 2.4420e-02,  1.2290e-01, -1.8334e-02],\n",
       "        [ 9.2891e-03,  1.0445e-01, -1.7147e-01],\n",
       "        [ 1.1851e-01, -7.2975e-02,  3.9864e-04],\n",
       "        [ 8.9081e-02, -1.9353e-02, -6.6153e-02],\n",
       "        [ 1.1634e-01,  1.4336e-02, -9.7661e-02],\n",
       "        [ 1.8058e-03, -4.5348e-02,  1.6860e-02],\n",
       "        [ 1.3435e-01, -1.8382e-01, -3.2829e-02],\n",
       "        [ 6.5727e-02,  1.4214e-01, -1.4354e-01],\n",
       "        [-1.2710e-01, -2.1706e-02, -3.1675e-02],\n",
       "        [-5.9282e-02, -6.1657e-03,  1.0446e-01],\n",
       "        [ 6.4310e-04,  1.6843e-01,  3.8096e-02],\n",
       "        [-4.4417e-02,  9.8962e-02, -1.3187e-01],\n",
       "        [ 1.3082e-01,  4.2218e-02,  2.5828e-01],\n",
       "        [-6.5845e-02,  2.0811e-02, -2.6181e-02],\n",
       "        [ 1.5026e-01,  2.1281e-01,  1.7660e-01],\n",
       "        [-5.2822e-02,  1.8102e-02, -1.5035e-01],\n",
       "        [-4.4512e-02, -3.8794e-02, -1.1524e-01],\n",
       "        [ 5.8785e-02, -1.4509e-01, -1.0118e-01],\n",
       "        [ 1.5249e-03, -3.7470e-02, -2.3168e-01],\n",
       "        [-1.0308e-01, -1.0147e-01,  3.6395e-02],\n",
       "        [-1.4204e-01,  1.3310e-01,  2.4245e-02],\n",
       "        [ 9.1594e-02,  6.2715e-02,  7.5297e-02],\n",
       "        [-2.4017e-01, -1.7396e-01,  6.9448e-02],\n",
       "        [ 6.5954e-02, -5.7206e-02,  7.0919e-03],\n",
       "        [-6.3640e-03,  1.8413e-01,  1.1555e-01],\n",
       "        [ 2.3627e-02, -8.0239e-02,  1.3745e-01],\n",
       "        [ 2.6607e-02, -9.8744e-02, -5.3068e-04],\n",
       "        [ 3.2815e-03,  4.4838e-02, -2.7001e-02],\n",
       "        [-1.5728e-01,  3.7401e-02, -8.0055e-02],\n",
       "        [ 3.5082e-02,  7.4520e-02,  2.2340e-01],\n",
       "        [-1.7855e-01,  6.2764e-02, -1.3943e-01],\n",
       "        [ 6.9124e-02, -1.1575e-01,  7.9829e-02],\n",
       "        [ 7.6864e-02,  3.2880e-02, -2.4518e-02],\n",
       "        [-7.1553e-02,  6.1717e-03,  1.4325e-01],\n",
       "        [ 4.6198e-02, -2.4172e-02, -1.9271e-01],\n",
       "        [-3.1696e-02,  9.7984e-02,  1.6898e-03],\n",
       "        [ 1.0202e-01, -1.1653e-01, -1.1655e-01],\n",
       "        [ 8.0361e-02,  1.8778e-01, -1.7867e-01],\n",
       "        [-1.4708e-01,  1.4292e-01, -1.1996e-02],\n",
       "        [ 5.9750e-02, -1.9948e-01,  1.7579e-01],\n",
       "        [ 1.2998e-01, -2.5649e-02,  4.4299e-02],\n",
       "        [-1.6920e-01,  7.7516e-02, -1.5109e-01],\n",
       "        [ 9.4873e-02, -6.2539e-02, -2.9621e-01]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a lstm with pytorch. the inputs should have lstom_layers=3, n_inputs=len(xcols), n_outputs=len(ycols) and n_timesteps=exp_data.X_lstm.shape[1], linear_layers=3, \n",
    "# linear_layer_size=50. create the entire model with the __init__ function, and then create the forward function.\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_lstm_layers=3, n_lstm_outputs=50, \n",
    "                 lstm_hidden_size=3, n_inputs=8, n_outputs=3, \n",
    "                 n_timesteps=204, n_linear_layers=1, linear_layer_size=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm_layers = n_lstm_layers\n",
    "        self.n_lstm_hidden_size = lstm_hidden_size\n",
    "        self.n_lstm_outputs = n_lstm_outputs\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_linear_layers = n_linear_layers\n",
    "        self.linear_layer_size = linear_layer_size\n",
    "        self.lstm_output_dim = n_timesteps*n_lstm_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=n_inputs, \n",
    "                            hidden_size=lstm_hidden_size, \n",
    "                            num_layers=n_lstm_layers)\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.first_linear_layer = nn.Linear(self.lstm_output_dim, linear_layer_size)\n",
    "        for i in range(n_linear_layers):\n",
    "            self.linear_layers.append(nn.Linear(linear_layer_size, linear_layer_size))\n",
    "        self.output_layer = nn.Linear(linear_layer_size, n_outputs*self.n_timesteps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # make sure x in flattened\n",
    "        x = x.flatten()\n",
    "        x = self.first_linear_layer(x)\n",
    "        for layer in self.linear_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x.reshape(self.n_timesteps, self.n_outputs)\n",
    "    \n",
    "model = LSTM()\n",
    "test = torch.tensor(exp_data.X_lstm[0:1])\n",
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH :  0 , dt:  44.19698405265808 seconds, losses : 1.7726176977157593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses \u001b[39m=\u001b[39m kmk\u001b[39m.\u001b[39;49mrun_Pytorch(model, data\u001b[39m.\u001b[39;49mX_lstm, data\u001b[39m.\u001b[39;49mY_lstm, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m21\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\kmodels\\kmodels.py:23\u001b[0m, in \u001b[0;36mrun_Pytorch\u001b[1;34m(model, X_train, Y_train, n_epochs, learning_rate, batch_size, device, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m optimizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m losses \u001b[39m=\u001b[39m train_pytorch(model, \n\u001b[0;32m     24\u001b[0m              X_train, \n\u001b[0;32m     25\u001b[0m              Y_train,\n\u001b[0;32m     26\u001b[0m              n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[0;32m     27\u001b[0m              batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m     28\u001b[0m              learning_rate\u001b[39m=\u001b[39;49mlearning_rate)\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\kmodels\\kmodels.py:68\u001b[0m, in \u001b[0;36mtrain_pytorch\u001b[1;34m(model, X_train, Y_train, n_epochs, batch_size, learning_rate, device, optimizer)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m loss_func \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m---> 68\u001b[0m losses \u001b[39m=\u001b[39m run_epochs(model, X_train, Y_train, loss_func, optimizer, batches, n_epochs\u001b[39m=\u001b[39;49mn_epochs)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m [i\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m losses]\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\kmodels\\kmodels.py:48\u001b[0m, in \u001b[0;36mrun_epochs\u001b[1;34m(model, X_train, Y_train, loss_func, optimizer, batches, n_epochs, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(pred, y) \u001b[39m# must be (1. nn output, 2. target)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     loss\u001b[39m.\u001b[39mbackward()         \u001b[39m# backpropagation, compute gradients\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()        \u001b[39m# apply gradients\u001b[39;00m\n\u001b[0;32m     49\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     50\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\efons\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\optim\\adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = kmk.run_Pytorch(model, data.X_lstm, data.Y_lstm, n_epochs=21, batch_size=1, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLLSTM(model, X, change_layers=1):\n",
    "    new = LSTM(n_lstm_layers=model.lstm_layers, n_lstm_outputs=model.n_lstm_outputs,\n",
    "               lstm_hidden_size=model.n_lstm_hidden_size, n_inputs=model.n_inputs,\n",
    "               n_timesteps=model.n_timesteps, n_linear_layers=model.n_linear_layers,\n",
    "               linear_layer_size=model.linear_layer_size)\n",
    "    test = new(X)\n",
    "    new.load_state_dict(model.state_dict())\n",
    "    children = [child for child in new.children()]\n",
    "    for child in children:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    total_layers = len(children)\n",
    "    for i in range(change_layers):\n",
    "        layer = children[total_layers-i-1]\n",
    "        layer_params = layer.parameters()\n",
    "        for p in layer_params:\n",
    "            p.requires_grad = True\n",
    "    return new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(8, 3, num_layers=3)\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  )\n",
       "  (first_linear_layer): Linear(in_features=612, out_features=50, bias=True)\n",
       "  (output_layer): Linear(in_features=50, out_features=612, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl_model = TLLSTM(model, test, change_layers=1)\n",
    "tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(data.X_lstm.to('cuda')).detach().cpu().numpy()\n",
    "pred[:,0].shape, data.Y[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = data.X[data.subject_index[0]]\n",
    "plt.plot(x_1[0:204,0], x_1[0:204,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.X.float()\n",
    "Y = data.Y.float()\n",
    "\n",
    "# use a scaler to scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = torch.from_numpy(X).float()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[0:300,0], X[0:300,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = kmk.run_Pytorch(model, X, Y, batch_size=len(X)/2, n_epochs=251, learning_rate=1e-3,\n",
    "                         optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = kmk.TLNN(model, X[0:2, :], change_layers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('../../data/SeparatedData.pkl')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in df.columns if i in xcols])\n",
    "print(xcols)\n",
    "exp_data = kmk.Dataset(df2, sort_column=['Subject'])\n",
    "n_subjects = len(exp_data.subjects)\n",
    "\n",
    "test_data = exp_data.X[(n_subjects-1)*900:, :]\n",
    "test_labels = exp_data.Y[(n_subjects-1)*900:, :]\n",
    "\n",
    "\n",
    "train_data= exp_data.X[0:(n_subjects-1)*900, :]\n",
    "train_labels = exp_data.Y[0:(n_subjects-1)*900, :]\n",
    "train_data = train_data.float()\n",
    "train_labels = train_labels.float()\n",
    "# scale the data\n",
    "train_data = train_data.detach().cpu().numpy()\n",
    "train_data = scaler.transform(train_data)\n",
    "train_data = torch.from_numpy(train_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_losses = kmk.run_Pytorch(tlm, train_data, train_labels, learning_rate=1e-3, \n",
    "                             batch_size=32, n_epochs=1001, \n",
    "                             optimizer=torch.optim.Adam(tlm.parameters(), lr=1e-3, weight_decay=1e-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parity plot\n",
    "pred = tlm(test_data.float().to('cuda'))\n",
    "plt.scatter(pred.detach().cpu().numpy()[:,0], test_labels.detach().cpu().numpy()[:,0])\n",
    "r2 = kmk.r2_score(pred.detach().cpu().numpy()[:,0], test_labels.detach().cpu().numpy()[:,0])\n",
    "mae = kmk.mean_absolute_error(pred.detach().cpu().numpy()[:,0], test_labels.detach().cpu().numpy()[:,0])\n",
    "mse = kmk.mean_squared_error(pred.detach().cpu().numpy()[:,0], test_labels.detach().cpu().numpy()[:,0])\n",
    "print('r2: ', r2)\n",
    "print('mae: ', mae)\n",
    "print('mse: ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for comparison\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators=1000, max_depth=10, random_state=0, n_jobs=-1)\n",
    "reg.fit(train_data.detach().cpu().numpy(), train_labels.detach().cpu().numpy())\n",
    "pred = reg.predict(test_data.detach().cpu().numpy())\n",
    "# parity plot\n",
    "plt.scatter(pred[:,0], test_labels.detach().cpu().numpy()[:,0])\n",
    "# print out all the stats\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "r2 = r2_score(test_labels.detach().cpu().numpy(), pred)\n",
    "mae = mean_absolute_error(test_labels.detach().cpu().numpy(), pred)\n",
    "mse = mean_squared_error(test_labels.detach().cpu().numpy(), pred)\n",
    "print('R2: ', r2)\n",
    "print('MAE', mae)\n",
    "print('MSE', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a linear model\n",
    "# grab the x data for the first subject\n",
    "x, y = kmk.Dataset(df2, sort_column=['Subject'])[0]\n",
    "# sort x and y by the time column\n",
    "time = x[:,0]\n",
    "x = x[time.argsort()]\n",
    "y = y[time.argsort()]\n",
    "plt.plot(x[:,0], y[:,0])\n",
    "plt.plot(x[:,0], y[:,1])\n",
    "plt.plot(x[:,0], y[:,2])\n",
    "\n",
    "# lets plot the predictions from the tlm model\n",
    "x_1 = scaler.transform(x)\n",
    "pred_1 = tlm(torch.from_numpy(x_1).float().to('cuda')).detach().cpu().numpy()\n",
    "x_1 = scaler.inverse_transform(x_1)\n",
    "plt.plot(x[:,0], pred_1[:,0],alpha=0.5)\n",
    "plt.plot(x[:,0], pred_1[:,1],alpha=0.5)\n",
    "plt.plot(x[:,0], pred_1[:,2],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2c1d3e12e66dc47a0aae61db106152a68be04014ca291cb297471e9102f95b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
